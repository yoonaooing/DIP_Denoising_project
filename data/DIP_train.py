# -*- coding: utf-8 -*-
"""DIP_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pPytdLOZsgObO-sn3FEX5jz-6L6FpmPh

###구글 드라이브 마운트
"""

from google.colab import drive
drive.mount('/content/drive')

"""###Utils.py"""

import math
import torch
import torch.nn as nn
import numpy as np
from skimage.metrics import peak_signal_noise_ratio

def weights_init_kaiming(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_in')
    elif classname.find('Linear') != -1:
        nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_in')
    elif classname.find('BatchNorm') != -1:
        # nn.init.uniform(m.weight.data, 1.0, 0.02)
        m.weight.data.normal_(mean=0, std=math.sqrt(2./9./64.)).clamp_(-0.025,0.025)
        nn.init.constant(m.bias.data, 0.0)

def batch_PSNR(img, imclean, data_range):
    Img = img.data.cpu().numpy().astype(np.float32)
    Iclean = imclean.data.cpu().numpy().astype(np.float32)
    PSNR = 0
    for i in range(Img.shape[0]):
        PSNR += peak_signal_noise_ratio(Iclean[i,:,:,:], Img[i,:,:,:], data_range=data_range)
    return (PSNR/Img.shape[0])

def data_augmentation(image, mode):
    out = np.transpose(image, (1,2,0))
    if mode == 0:
        # original
        out = out
    elif mode == 1:
        # flip up and down
        out = np.flipud(out)
    elif mode == 2:
        # rotate counterwise 90 degree
        out = np.rot90(out)
    elif mode == 3:
        # rotate 90 degree and flip up and down
        out = np.rot90(out)
        out = np.flipud(out)
    elif mode == 4:
        # rotate 180 degree
        out = np.rot90(out, k=2)
    elif mode == 5:
        # rotate 180 degree and flip
        out = np.rot90(out, k=2)
        out = np.flipud(out)
    elif mode == 6:
        # rotate 270 degree
        out = np.rot90(out, k=3)
    elif mode == 7:
        # rotate 270 degree and flip
        out = np.rot90(out, k=3)
        out = np.flipud(out)
    return np.transpose(out, (2,0,1))

"""##Preprocessing (.h5)

### 기존의 전처리 코드에서 파일 로드가 되지 않아 별도로 진행하였음.
###Train.h5
###Valid.h5
"""

import os
import cv2
import h5py
import numpy as np
from glob import glob

def normalize(data):
    return data / 255.

def create_h5(data_path, output_file='train.h5', patch_size=40, stride=10):
    files = glob(os.path.join(data_path, '*.png'))
    print(f"총 이미지 수: {len(files)}")

    data = []
    for f in files:
        img = cv2.imread(f, 0)  # grayscale
        if img is None:
            print(f"❌ {f} 로딩 실패")
            continue
        h, w = img.shape
        for i in range(0, h - patch_size + 1, stride):
            for j in range(0, w - patch_size + 1, stride):
                patch = img[i:i+patch_size, j:j+patch_size]
                if patch.shape != (patch_size, patch_size):
                    continue
                patch = normalize(patch)  # 정규화 추가
                data.append(patch)

    data = np.array(data, dtype=np.float32)  # float32로 바꾸기
    data = np.expand_dims(data, axis=1)  # (N, 1, 40, 40)
    print(f"총 패치 수: {data.shape[0]}")

    with h5py.File(output_file, 'w') as hf:
        for i in range(len(data)):
            hf.create_dataset(str(i), data=data[i])

    print(f"{output_file} 저장 완료")
# 실행
create_h5('/content/drive/MyDrive/DIP/data/train', patch_size=40, stride=10)

import os
import cv2
import h5py
import numpy as np
from glob import glob

def normalize(data):
    return data / 255.

def create_h5(data_path, output_file='val.h5', patch_size=40, stride=10):
    files = glob(os.path.join(data_path, '*.png'))
    print(f"총 이미지 수: {len(files)}")

    data = []
    for f in files:
        img = cv2.imread(f, 0)  # grayscale
        if img is None:
            print(f"❌ {f} 로딩 실패")
            continue
        h, w = img.shape
        for i in range(0, h - patch_size + 1, stride):
            for j in range(0, w - patch_size + 1, stride):
                patch = img[i:i+patch_size, j:j+patch_size]
                if patch.shape != (patch_size, patch_size):
                    continue
                patch = normalize(patch)  # 정규화
                data.append(patch)

    data = np.array(data, dtype=np.float32)
    data = np.expand_dims(data, axis=1)  # (N, 1, 40, 40)
    print(f"총 패치 수: {data.shape[0]}")

    with h5py.File(output_file, 'w') as hf:
        for i in range(len(data)):
            hf.create_dataset(str(i), data=data[i])

    print(f"{output_file} 저장 완료")
# 실행
create_h5('/content/drive/MyDrive/DIP/data/Set12', patch_size=40, stride=10)

"""##Model.py"""

import torch
import torch.nn as nn

class DnCNN(nn.Module):
    def __init__(self, channels, num_of_layers=18):
        super(DnCNN, self).__init__()
        kernel_size = 3
        padding = 1
        features = 128
        layers = []
        layers.append(nn.Conv2d(in_channels=channels, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))
        layers.append(nn.ReLU(inplace=True))
        for _ in range(num_of_layers-2):
            layers.append(nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))
            layers.append(nn.BatchNorm2d(features))
            layers.append(nn.ReLU(inplace=True))
        layers.append(nn.Conv2d(in_channels=features, out_channels=channels, kernel_size=kernel_size, padding=padding, bias=False))
        self.dncnn = nn.Sequential(*layers)
    def forward(self, x):
        out = self.dncnn(x)
        return out

"""##Dataset.py
## 앞에서 진행한 preprocessing 과정은 주석처리하였음.
"""

import os
import os.path
import numpy as np
import random
import h5py
import torch
import cv2
import glob
import torch.utils.data as udata

#from utils import data_augmentation

def normalize(data):
    return data/255.

""""
def Im2Patch(img, win, stride=1):
    k = 0
    endc = img.shape[0]
    endw = img.shape[1]
    endh = img.shape[2]
    patch = img[:, 0:endw-win+0+1:stride, 0:endh-win+0+1:stride]
    TotalPatNum = patch.shape[1] * patch.shape[2]
    Y = np.zeros([endc, win*win,TotalPatNum], np.float32)
    for i in range(win):
        for j in range(win):
            patch = img[:,i:endw-win+i+1:stride,j:endh-win+j+1:stride]
            Y[:,k,:] = np.array(patch[:]).reshape(endc, TotalPatNum)
            k = k + 1
    return Y.reshape([endc, win, win, TotalPatNum])

def prepare_data(data_path, patch_size, stride, aug_times=1):
    # train
    print('process training data')
    scales = [1, 0.9, 0.8, 0.7]
    files = glob.glob(os.path.join(data_path, 'train', '*.png'))
    files.sort()
    h5f = h5py.File('train.h5', 'w')
    train_num = 0
    for i in range(len(files)):
        img = cv2.imread(files[i])
        h, w, c = img.shape
        for k in range(len(scales)):
            Img = cv2.resize(img, (int(h*scales[k]), int(w*scales[k])), interpolation=cv2.INTER_CUBIC)
            Img = np.expand_dims(Img[:,:,0].copy(), 0)
            Img = np.float32(normalize(Img))
            patches = Im2Patch(Img, win=patch_size, stride=stride)
            print("file: %s scale %.1f # samples: %d" % (files[i], scales[k], patches.shape[3]*aug_times))
            for n in range(patches.shape[3]):
                data = patches[:,:,:,n].copy()
                h5f.create_dataset(str(train_num), data=data)
                train_num += 1
                for m in range(aug_times-1):
                    data_aug = data_augmentation(data, np.random.randint(1,8))
                    h5f.create_dataset(str(train_num)+"_aug_%d" % (m+1), data=data_aug)
                    train_num += 1
    h5f.close()
    # val
    print('\nprocess validation data')
    files.clear()
    files = glob.glob(os.path.join(data_path, 'Set12', '*.png'))
    files.sort()
    h5f = h5py.File('val.h5', 'w')
    val_num = 0
    for i in range(len(files)):
        print("file: %s" % files[i])
        img = cv2.imread(files[i])
        img = np.expand_dims(img[:,:,0], 0)
        img = np.float32(normalize(img))
        h5f.create_dataset(str(val_num), data=img)
        val_num += 1
    h5f.close()
    print('training set, # samples %d\n' % train_num)
    print('val set, # samples %d\n' % val_num)"""

class Dataset(udata.Dataset):
    def __init__(self, train=True):
        super(Dataset, self).__init__()
        self.train = train
        if self.train:
            h5f = h5py.File('train.h5', 'r')
        else:
            h5f = h5py.File('val.h5', 'r')
        self.keys = list(h5f.keys())
        random.shuffle(self.keys)
        h5f.close()
    def __len__(self):
        return len(self.keys)
    def __getitem__(self, index):
        if self.train:
            h5f = h5py.File('train.h5', 'r')
        else:
            h5f = h5py.File('val.h5', 'r')
        key = self.keys[index]
        data = np.array(h5f[key])
        h5f.close()
        return torch.Tensor(data)

"""##Train.py
###class Args 새로 정의하고 기존 코드 주석 처리함.
###Early Stopping 코드 추가함.
###sigma-15, 25, 50 세 가지 value에 대한 모델 학습하여 저장함.
"""

import os
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.utils as utils
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from skimage.metrics import peak_signal_noise_ratio
from skimage.metrics import structural_similarity as compare_ssim
#from models import DnCNN

#from dataset import prepare_data, Dataset
#from utils import *

os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
"""
parser = argparse.ArgumentParser(description="DnCNN")
parser.add_argument("--preprocess", type=bool, default=False, help='run prepare_data or not')
parser.add_argument("--batchSize", type=int, default=128, help="Training batch size")
parser.add_argument("--num_of_layers", type=int, default=17, help="Number of total layers")
parser.add_argument("--epochs", type=int, default=50, help="Number of training epochs")
parser.add_argument("--milestone", type=int, default=30, help="When to decay learning rate; should be less than epochs")
parser.add_argument("--lr", type=float, default=1e-3, help="Initial learning rate")
parser.add_argument("--outf", type=str, default="logs", help='path of log files')
parser.add_argument("--mode", type=str, default="S", help='with known noise level (S) or blind training (B)')
parser.add_argument("--noiseL", type=float, default=25, help='noise level; ignored when mode=B')
parser.add_argument("--val_noiseL", type=float, default=25, help='noise level used on validation set')
opt = parser.parse_args()"""
class EarlyStopping:
    def __init__(self, patience=5, delta=0.005):
        self.patience = patience
        self.delta = delta
        self.best_score = None
        self.counter = 0
        self.early_stop = False

    def __call__(self, current_score):
        if self.best_score is None or current_score > self.best_score + self.delta:
            self.best_score = current_score
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        return self.early_stop

class Args:
    preprocess = False
    batchSize = 128
    num_of_layers = 18
    epochs = 50
    milestone = 30
    lr = 1e-3
    outf = "logs"
    mode = "S"
    noiseL = 15
    val_noiseL = 15

opt = Args()

def main():
    # Load dataset
    print('Loading dataset ...\n')
    dataset_train = Dataset(train=True)
    dataset_val = Dataset(train=False)
    opt.outf = "/content/drive/MyDrive/DnCNN_logs/exp50"
    os.makedirs(opt.outf, exist_ok=True)
    writer = SummaryWriter(log_dir=opt.outf)

    loader_train = DataLoader(dataset=dataset_train, num_workers=4, batch_size=opt.batchSize, shuffle=True)
    print("# of training samples: %d\n" % int(len(dataset_train)))
    # Build model
    net = DnCNN(channels=1, num_of_layers=opt.num_of_layers)
    net.apply(weights_init_kaiming)
    criterion = nn.MSELoss(size_average=False)
    # Move to GPU
    device_ids = [0]
    model = nn.DataParallel(net, device_ids=device_ids).cuda()
    criterion.cuda()
    # Optimizer
    optimizer = optim.Adam(model.parameters(), lr=opt.lr)
    # training
    step = 0
    noiseL_B=[0,55] # ingnored when opt.mode=='S'

    early_stopping = EarlyStopping(patience=5, delta=0.01)
    for epoch in range(opt.epochs):
        if epoch < opt.milestone:
            current_lr = opt.lr
        else:
            current_lr = opt.lr / 10.
        # set learning rate
        for param_group in optimizer.param_groups:
            param_group["lr"] = current_lr
        print('learning rate %f' % current_lr)
        # train
        for i, data in enumerate(loader_train, 0):
            # training step
            model.train()
            model.zero_grad()
            optimizer.zero_grad()
            img_train = data
            if opt.mode == 'S':
                noise = torch.FloatTensor(img_train.size()).normal_(mean=0, std=opt.noiseL/255.)
            if opt.mode == 'B':
                noise = torch.zeros(img_train.size())
                stdN = np.random.uniform(noiseL_B[0], noiseL_B[1], size=noise.size()[0])
                for n in range(noise.size()[0]):
                    sizeN = noise[0,:,:,:].size()
                    noise[n,:,:,:] = torch.FloatTensor(sizeN).normal_(mean=0, std=stdN[n]/255.)
            imgn_train = img_train + noise
            img_train = img_train.cuda()
            imgn_train = imgn_train.cuda()
            noise = noise.cuda()
            out_train = model(imgn_train)
            loss = criterion(out_train, noise) / (imgn_train.size()[0]*2)
            loss.backward()
            optimizer.step()
            # results
            model.eval()
            out_train = torch.clamp(imgn_train-model(imgn_train), 0., 1.)
            psnr_train = batch_PSNR(out_train, img_train, 1.)

            print("[epoch %d][%d/%d] loss: %.4f PSNR_train: %.4f" %
                (epoch+1, i+1, len(loader_train), loss.item(), psnr_train))
            # if you are using older version of PyTorch, you may need to change loss.item() to loss.data[0]
            if step % 10 == 0:
                # Log the scalar values
                writer.add_scalar('loss', loss.item(), step)
                writer.add_scalar('PSNR on training data', psnr_train, step)
            step += 1
        ## the end of each epoch
        model.eval()
        # validate
        psnr_val = 0
        ssim_val = 0
        for k in range(len(dataset_val)):
            img_val = torch.unsqueeze(dataset_val[k], 0)  # shape: (1, 1, 40, 40)
            noise = torch.FloatTensor(img_val.size()).normal_(mean=0, std=opt.val_noiseL/255.)
            imgn_val = img_val + noise

            # 2. GPU로 이동
            img_val = img_val.cuda()
            imgn_val = imgn_val.cuda()

            # 3. 모델 추론 (gradient 끄기)
            with torch.no_grad():
                out_val = torch.clamp(imgn_val - model(imgn_val), 0., 1.)

            # 4. PSNR 계산
            psnr_val += batch_PSNR(out_val, img_val, 1.)

            # 5. SSIM 계산 (detach로 그래프 분리하고 CPU로 이동 후 numpy 변환)
            img_np = img_val.squeeze().detach().cpu().numpy()
            out_np = out_val.squeeze().detach().cpu().numpy()
            ssim_val += compare_ssim(img_np, out_np, data_range=1.0)


        psnr_val /= len(dataset_val)
        ssim_val /= len(dataset_val)
        print(f"[epoch {epoch+1}] PSNR_val: {psnr_val:.4f}, SSIM_val: {ssim_val:.4f}")
        writer.add_scalar('PSNR on validation data', psnr_val, epoch)
        writer.add_scalar('SSIM on validation data', ssim_val, epoch)
        # log the images
        #out_train = torch.clamp(imgn_train-model(imgn_train), 0., 1.)
        Img = utils.make_grid(img_train.detach(), nrow=8, normalize=True, scale_each=True)
        Imgn = utils.make_grid(imgn_train.detach(), nrow=8, normalize=True, scale_each=True)
        Irecon = utils.make_grid(out_train.detach(), nrow=8, normalize=True, scale_each=True)
        writer.add_image('clean image', Img, epoch)
        writer.add_image('noisy image', Imgn, epoch)
        writer.add_image('reconstructed image', Irecon, epoch)
        # save model
        save_path = '/content/drive/MyDrive/DnCNN_models'
        os.makedirs(save_path, exist_ok=True)
        torch.save(model.state_dict(), os.path.join(save_path, 'net_train_50.pth'))

        if early_stopping(psnr_val):
            print(f"Early stopping triggered at epoch {epoch+1}")
            writer.close()
            break


if __name__ == "__main__":
    if opt.preprocess:
        if opt.mode == 'S':
            prepare_data(data_path='data', patch_size=40, stride=10, aug_times=1)
        if opt.mode == 'B':
            prepare_data(data_path='data', patch_size=50, stride=10, aug_times=2)
    main()

"""##Test code
##모델 구조 확인용 코드
##테스트 코드
"""

!ls "/content/drive/MyDrive/DnCNN_models/"

import torch
from collections import OrderedDict
import torch.nn as nn

class DnCNN(nn.Module):
    def __init__(self, channels, num_of_layers=18):
        super(DnCNN, self).__init__()
        kernel_size = 3
        padding = 1
        features = 128
        layers = []
        layers.append(nn.Conv2d(in_channels=channels, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))
        layers.append(nn.ReLU(inplace=True))
        for _ in range(num_of_layers-2):
            layers.append(nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))
            layers.append(nn.BatchNorm2d(features))
            layers.append(nn.ReLU(inplace=True))
        layers.append(nn.Conv2d(in_channels=features, out_channels=channels, kernel_size=kernel_size, padding=padding, bias=False))
        self.dncnn = nn.Sequential(*layers)

    def forward(self, x):
        out = self.dncnn(x)
        return out

# 1. 모델 구조 정의 (channels와 num_of_layers 정확히 일치해야 함)
model = DnCNN(channels=1, num_of_layers=18)

# 2. 학습된 weight 불러오기
state_dict = torch.load('/content/drive/MyDrive/DnCNN_models/net_train_50.pth', map_location='cuda')

# 3. DataParallel prefix "module." 제거
new_state_dict = OrderedDict()
for k, v in state_dict.items():
    name = k.replace('module.', '')  # 'module.' 제거
    new_state_dict[name] = v

# 4. 모델에 적용
model.load_state_dict(new_state_dict)

# 5. 평가 모드로 전환
model.eval()

# 6. 필요 시 GPU로 이동 (선택사항)
# model = model.cuda()

import cv2
import os
import argparse
import glob
import numpy as np
import torch
import torch.nn as nn
from torch.autograd import Variable
from skimage.metrics import structural_similarity as compare_ssim
#from models import DnCNN
#from utils import *

os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
"""
parser = argparse.ArgumentParser(description="DnCNN_Test")
parser.add_argument("--num_of_layers", type=int, default=17, help="Number of total layers")
parser.add_argument("--logdir", type=str, default="logs", help='path of log files')
parser.add_argument("--test_data", type=str, default='Set12', help='test on Set12 or Set68')
parser.add_argument("--test_noiseL", type=float, default=25, help='noise level used on test set')
opt = parser.parse_args()
"""

class Args:
    preprocess = False
    batchSize = 128
    num_of_layers = 18
    epochs = 50
    milestone = 30
    lr = 1e-3
    outf = "logs"
    mode = "S"
    noiseL = 25
    val_noiseL = 25
    test_noiseL = 25
    test_data = '/content/drive/MyDrive/DIP/data/BSD68/original/'

opt = Args()

def normalize(data):
    return data/255.

def main():
    # Build model
    print('Loading model ...\n')
    model = DnCNN(channels=1, num_of_layers=18)
    state_dict = torch.load('/content/drive/MyDrive/DnCNN_models/net_train_25.pth', map_location='cpu')
    new_state_dict = OrderedDict((k.replace('module.', ''), v) for k, v in state_dict.items())
    model.load_state_dict(new_state_dict)
    model = model.cuda()
    model.eval()
    # load data info
    print('Loading data info ...\n')
    files_source = glob.glob(os.path.join(opt.test_data, '*.png'))
    files_source.sort()
    # process data
    psnr_test = 0
    ssim_test = 0

    for f in files_source:
        # image
        Img = cv2.imread(f)
        Img = normalize(np.float32(Img[:,:,0]))
        Img = np.expand_dims(Img, 0)
        Img = np.expand_dims(Img, 1)
        ISource = torch.Tensor(Img)
        # noise
        noise = torch.FloatTensor(ISource.size()).normal_(mean=0, std=opt.test_noiseL/255.)
        # noisy image
        INoisy = ISource + noise
        ISource, INoisy = Variable(ISource.cuda()), Variable(INoisy.cuda())
        with torch.no_grad(): # this can save much memory
            output = model(INoisy)
        Out = torch.clamp(INoisy-output, 0., 1.)
        ## if you are using older version of PyTorch, torch.no_grad() may not be supported
        # ISource, INoisy = Variable(ISource.cuda(),volatile=True), Variable(INoisy.cuda(),volatile=True)
        # Out = torch.clamp(INoisy-model(INoisy), 0., 1.)
        psnr = batch_PSNR(Out, ISource, 1.)
        psnr_test += psnr
        out_np = Out.squeeze().cpu().numpy()
        gt_np = ISource.squeeze().cpu().numpy()
        ssim = compare_ssim(gt_np, out_np, data_range=1.0)
        ssim_test += ssim

        print(f"{f} | PSNR: {psnr:.4f} | SSIM: {ssim:.4f}")
    psnr_test /= len(files_source)
    ssim_test /= len(files_source)
    print("\nPSNR on test data %f" % psnr_test)
    print(f"Average SSIM: {ssim_test:.4f}")

if __name__ == "__main__":
    main()

"""##이미지 시각화"""

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from collections import OrderedDict
from skimage.metrics import peak_signal_noise_ratio, structural_similarity

# DnCNN 모델 정의
class DnCNN(nn.Module):
    def __init__(self, channels, num_of_layers=18):
        super(DnCNN, self).__init__()
        kernel_size = 3
        padding = 1
        features = 128
        layers = []
        layers.append(nn.Conv2d(in_channels=channels, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))
        layers.append(nn.ReLU(inplace=True))
        for _ in range(num_of_layers - 2):
            layers.append(nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))
            layers.append(nn.BatchNorm2d(features))
            layers.append(nn.ReLU(inplace=True))
        layers.append(nn.Conv2d(in_channels=features, out_channels=channels, kernel_size=kernel_size, padding=padding, bias=False))
        self.dncnn = nn.Sequential(*layers)

    def forward(self, x):
        out = self.dncnn(x)
        return out

# 1. 경로 설정 및 25번째 이미지 선택
img_dir = '/content/drive/MyDrive/DIP/data/BSD68/original/'
img_paths = sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith('.png')])
img_path = img_paths[24]  # 25번째 이미지

# 2. 이미지 로딩 및 노이즈 추가
img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.0
noise_sigma = 15 / 255.0
noisy = np.clip(img + np.random.normal(0, noise_sigma, img.shape), 0., 1.)

# 3. 모델 로딩
model = DnCNN(channels=1, num_of_layers=18)
state_dict = torch.load('/content/drive/MyDrive/DnCNN_models/net_train_25.pth', map_location='cpu')
new_state_dict = OrderedDict((k.replace('module.', ''), v) for k, v in state_dict.items())
model.load_state_dict(new_state_dict)
model.eval()

# 4. 추론 (Residual Learning 방식 적용)
noisy_tensor = torch.from_numpy(noisy).unsqueeze(0).unsqueeze(0).float()
with torch.no_grad():
    residual = model(noisy_tensor)
    denoised_tensor = noisy_tensor - residual
denoised = denoised_tensor.squeeze().cpu().numpy()
denoised = np.clip(denoised, 0., 1.)

# 5. PSNR / SSIM
psnr_noisy = peak_signal_noise_ratio(img, noisy)
psnr_denoised = peak_signal_noise_ratio(img, denoised)
ssim_noisy = structural_similarity(img, noisy, data_range=1.0)
ssim_denoised = structural_similarity(img, denoised, data_range=1.0)

# 6. 결과 시각화
plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
plt.imshow(img, cmap='gray')
plt.title('Original')
plt.axis('off')

plt.subplot(1, 3, 2)
plt.imshow(noisy, cmap='gray')
plt.title(f'Noisy (σ=25)\nPSNR: {psnr_noisy:.2f}')
plt.axis('off')

plt.subplot(1, 3, 3)
plt.imshow(denoised, cmap='gray')
plt.title(f'Denoised\nPSNR: {psnr_denoised:.2f}')
plt.axis('off')

plt.tight_layout()
plt.show()

# 7. 수치 출력
print(f"Noisy PSNR:    {psnr_noisy:.4f}")
print(f"Denoised PSNR: {psnr_denoised:.4f}")
print(f"Noisy SSIM:    {ssim_noisy:.4f}")
print(f"Denoised SSIM: {ssim_denoised:.4f}")

"""##Tensorboard 그래프 로그 시각화"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir "/content/drive/MyDrive/DnCNN_logs/exp50"

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir "/content/drive/MyDrive/DnCNN_logs/exp25"

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir "/content/drive/MyDrive/DnCNN_logs/exp15"
